{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports",
   "id": "80310ea8c9f34b62"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T01:39:01.577330Z",
     "start_time": "2026-02-21T01:39:01.566068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "from datetime import datetime\n",
    "import random\n",
    "import re\n",
    "\n",
    "EOS = '<EOS>'"
   ],
   "id": "1d766638a5a0fe6a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Models",
   "id": "d59d3fe71708101e"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-21T01:40:05.045836Z",
     "start_time": "2026-02-21T01:40:04.970568Z"
    }
   },
   "source": [
    "class Model():\n",
    "    input_weights = None\n",
    "    output_weights = None\n",
    "\n",
    "    negative_samples = 5\n",
    "\n",
    "    nwords_per_line = 10\n",
    "    min_words_threshold = 3\n",
    "\n",
    "    vocabulary = None\n",
    "    vocabulary_size = None\n",
    "    vocabulary_index_map = None\n",
    "\n",
    "    is_inited = False\n",
    "\n",
    "    def __init__(self, hidden_size=100, window_size=4, weight_init_seed=31, alpha=0.01):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.window_size = window_size\n",
    "        self.weight_init_seed = weight_init_seed\n",
    "        self.alpha = alpha\n",
    "        self.loss_model = Loss(self, self.alpha)\n",
    "\n",
    "    def initialise_weights(self, seed=31):\n",
    "        if self.vocabulary_size is None:\n",
    "            raise ValueError(\"Unknown Vocabulary. Run Preprocessing.\")\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        if self.input_weights is None:\n",
    "            self.input_weights = np.random.uniform(low=-0.05, high=0.05, size=(self.vocabulary_size, self.hidden_size))\n",
    "        if self.output_weights is None:\n",
    "            self.output_weights = np.random.uniform(low=-0.05, high=0.05, size=(self.vocabulary_size, self.hidden_size))\n",
    "        self.is_inited = True\n",
    "\n",
    "    def __input_label_gen__(self, data: list[str]):\n",
    "        for sample in data:\n",
    "            tokens = [token for token in sample.split() if self.vocabulary_index_map.get(token, None) is not None]\n",
    "            ntokens = len(tokens)\n",
    "            for idx in range(ntokens):\n",
    "                target_idx = self.vocabulary_index_map.get(tokens[idx])\n",
    "                Y = target_idx\n",
    "                X = []\n",
    "                if idx == 0:\n",
    "                    for i in range(1, min(ntokens, self.window_size + 1)):\n",
    "                        label_idx = self.vocabulary_index_map.get(tokens[i])\n",
    "                        X.append(label_idx)\n",
    "                        yield np.array(X), Y\n",
    "                elif idx == ntokens - 1:\n",
    "                    for i in range(1, min(ntokens, self.window_size + 1)):\n",
    "                        label_idx = self.vocabulary_index_map.get(tokens[idx - i])\n",
    "                        X.append(label_idx)\n",
    "                        yield np.array(X), Y\n",
    "                else:\n",
    "                    left = max(0, idx - self.window_size)\n",
    "                    right = min(ntokens, idx + self.window_size)\n",
    "                    X.extend([self.vocabulary_index_map.get(idx) for idx in tokens[left:idx]])\n",
    "                    X.extend([self.vocabulary_index_map.get(idx) for idx in tokens[idx + 1:right + 1]])\n",
    "                    yield np.array(X), Y\n",
    "\n",
    "    def preprocess(self, f: list[str], first: int = 5000, train: bool = False, update_vocab: bool = False):\n",
    "        nlines = 0\n",
    "        ndigits = 0\n",
    "        digits = re.compile(r\"(\\d+)\")\n",
    "        unwanted_chars = re.compile(r\"([^\\d\\w ])\")\n",
    "        all_words = []\n",
    "        data = []\n",
    "\n",
    "        for line in f:\n",
    "            if \"html\" in line:\n",
    "                continue\n",
    "\n",
    "            if nlines == first:\n",
    "                break\n",
    "\n",
    "            line = re.sub(unwanted_chars, \"\", line)\n",
    "            digits_in_line = len(re.findall(digits, line))\n",
    "            line = re.sub(digits, \" ## \", line).lower()\n",
    "            line = re.sub(\" {2,}\", \" \", line).strip()\n",
    "            words = line.split()\n",
    "\n",
    "            if len(words) >= self.nwords_per_line or not train:\n",
    "                nlines += 1\n",
    "                ndigits += digits_in_line\n",
    "                all_words.extend(words)\n",
    "                data.append(line + \" \" + EOS)\n",
    "\n",
    "        all_words.append(EOS)\n",
    "\n",
    "        # Return pre-processed data when mode is not in train\n",
    "        if not train:\n",
    "            print(\"Words =\", len(all_words), \"Lines =\", nlines, \"Digits =\", ndigits)\n",
    "            return data\n",
    "\n",
    "        count = Counter(all_words)\n",
    "        count.__setitem__(EOS, nlines)\n",
    "\n",
    "        if self.vocabulary is None or update_vocab:\n",
    "            unique_all_words = sorted(set(all_words))\n",
    "            self.vocabulary = tuple(word for word in unique_all_words if count[word] > self.min_words_threshold)\n",
    "            self.vocabulary_size = len(self.vocabulary)\n",
    "            self.vocabulary_index_map = dict(zip(self.vocabulary, range(self.vocabulary_size)))\n",
    "\n",
    "        data = self.__remove_less_freq_words__(data, count)\n",
    "\n",
    "        print(\"Words =\", len(all_words), \"Vocab size =\", self.vocabulary_size, \"Lines =\", nlines, \"Digits =\", ndigits)\n",
    "        return data\n",
    "\n",
    "    def __remove_less_freq_words__(self, data: list[str], count: Counter | dict) -> list[str]:\n",
    "        dataset = []\n",
    "        for line in data:\n",
    "            cleaned = [word for word in line.split() if count[word] > self.min_words_threshold]\n",
    "            if len(cleaned) >= self.nwords_per_line:\n",
    "                dataset.append(\" \".join(cleaned))\n",
    "        return dataset\n",
    "\n",
    "    def fit(self, data: list[str]):\n",
    "        try:\n",
    "            if not self.is_inited:\n",
    "                self.initialise_weights()\n",
    "        except ValueError as e:\n",
    "            raise e\n",
    "\n",
    "        total_loss = 0\n",
    "        n = 0\n",
    "\n",
    "        # Streaming input\n",
    "        for x, y in self.__input_label_gen__(data):\n",
    "            # forward pass\n",
    "            # print(x, y)\n",
    "            loss = self.__fit(x, y)\n",
    "            # print(f\"Loss per example: {loss / (self.negative_samples + 1)}\")\n",
    "\n",
    "            total_loss += loss\n",
    "            n += self.negative_samples + 1\n",
    "\n",
    "        print(f\"Average Loss: {total_loss / n:.3f}\")\n",
    "\n",
    "    def __fit(self, x: ndarray, y):\n",
    "        total_loss = 0\n",
    "\n",
    "        # Projection layer ->\n",
    "        vectors = np.array([self.input_weights[idx] for idx in x])\n",
    "        x_proj = np.average(vectors, axis=0)\n",
    "\n",
    "        # Output layer ->\n",
    "        # Positive Sampling\n",
    "        z = x_proj.dot(self.output_weights[y])\n",
    "        pos_y_pred = sigmoid(z)\n",
    "\n",
    "        # Update Wout for positive sample\n",
    "        loss = self.loss_model.calculate_loss(pos_y_pred, y, x_proj, 1)\n",
    "        total_loss += loss\n",
    "\n",
    "        # Negative Sampling and updating Wout\n",
    "        for i in range(self.negative_samples):\n",
    "            neg_y = get_neg_sample(y, self.vocabulary_size - 1)\n",
    "\n",
    "            z = x_proj.dot(self.output_weights[neg_y])\n",
    "            neg_y_pred = sigmoid(z)\n",
    "\n",
    "            # Update Wout for negative sample\n",
    "            loss = self.loss_model.calculate_loss(neg_y_pred, neg_y, x_proj, 0)\n",
    "            total_loss += loss\n",
    "\n",
    "        # Update Win\n",
    "        self.loss_model.optimise(x)\n",
    "\n",
    "        # Reset gradients for next sample\n",
    "        self.loss_model.reset_grad()\n",
    "\n",
    "        # print(f\"Average Sample Loss: {total_loss / (self.negative_samples + 1)}\")\n",
    "        return total_loss\n",
    "\n",
    "    def predict(self, x: list[int]):\n",
    "\n",
    "        # Projection layer ->\n",
    "        vectors = np.array([self.input_weights[idx] for idx in x])\n",
    "        x_proj = np.average(vectors, axis=0)\n",
    "\n",
    "        # Output layer ->\n",
    "        # Positive Sampling\n",
    "        z = x_proj @ self.output_weights.T\n",
    "\n",
    "        # Returns the idx of highest score from matrix multiplication.\n",
    "        # Doesn't mean probabilities or confidence unlike softmax\n",
    "        y_pred = z.argmax()\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "class Loss():\n",
    "    def __init__(self, model: Model, alpha):\n",
    "        self.model = model\n",
    "        self.grad_win = np.zeros(model.hidden_size, dtype=float)\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.last_grad_win = np.zeros(model.hidden_size, dtype=float)\n",
    "        self.last_grad_wout = np.zeros(model.hidden_size, dtype=float)\n",
    "\n",
    "    def calculate_loss(self, y_pred: float, target_idx: int, x_proj: ndarray, label: int):\n",
    "        loss = - (label * np.log(y_pred + 1e-7)) - ((1 - label) * np.log(1 - y_pred + 1e-7))  # +1e-7 to prevent log(0)\n",
    "\n",
    "        err = self.alpha * (label - y_pred)  # Chain rule -> y_pred - label but to descent the slope, label - y_pred\n",
    "\n",
    "        self.grad_win += err * self.model.output_weights[target_idx]  # Accumulate gradients for Win\n",
    "\n",
    "        grad_wout = err * x_proj  # gradients for Wout\n",
    "        self.last_grad_wout = grad_wout  # storing last gradient for reference\n",
    "        self.model.output_weights[target_idx] += grad_wout  # Update Wout\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def optimise(self, x: ndarray):\n",
    "        self.grad_win = self.grad_win / len(x)\n",
    "        self.last_grad_win = self.grad_win  # storing last gradient for reference\n",
    "\n",
    "        for idx in x:\n",
    "            self.model.input_weights[idx] += self.grad_win\n",
    "\n",
    "    def reset_grad(self):\n",
    "        self.grad_win = np.zeros(self.model.hidden_size, dtype=float)\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def get_neg_sample(y, max):\n",
    "    random.seed(datetime.now().timestamp())\n",
    "    # Uniform sampling but original implementation uses distribution that is inverse to the frequency of the tokens\n",
    "    neg_y = random.randint(0, max)\n",
    "    while neg_y == y:\n",
    "        neg_y = random.randint(0, max)\n",
    "    return neg_y\n",
    "\n",
    "\n",
    "def train_test_split(data, seed):\n",
    "    random.seed(seed)\n",
    "    random.shuffle(data)\n",
    "    train_size = round(len(data) * 0.8)\n",
    "    train = data[0:train_size]\n",
    "    test = data[train_size:]\n",
    "    print(\"Train - \", len(train), \"Test - \", len(test))\n",
    "    return train, test"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "4698d9a5293a0dac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T01:41:27.086751Z",
     "start_time": "2026-02-21T01:41:26.915538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = []\n",
    "with open(\"../datasets/news.2025.en.shuffled.deduped\") as f:\n",
    "    lines = 0\n",
    "    for line in f:\n",
    "        data.append(line)\n",
    "        lines += 1\n",
    "        if lines == 5000:\n",
    "            break\n",
    "cbow = Model(hidden_size=50, alpha=0.01)\n",
    "processed_data = cbow.preprocess(data, train=True)"
   ],
   "id": "2f865ba616205c06",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words = 116994 Vocab size = 3733 Lines = 4280 Digits = 2016\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T01:41:32.355211Z",
     "start_time": "2026-02-21T01:41:32.332005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # View random samples from vocabulary map\n",
    "# random.sample(list(cbow.vocabulary_index_map.items()), 20)"
   ],
   "id": "ed21ad32cb6a115a",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T01:44:28.094805Z",
     "start_time": "2026-02-21T01:41:34.415161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "epoch = 10\n",
    "for i in range(1, epoch + 1):\n",
    "    print(\"Epoch: \", i)\n",
    "    cbow.fit(processed_data)"
   ],
   "id": "5c801603739180fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "Average Loss: 0.601\n",
      "Epoch:  2\n",
      "Average Loss: 0.344\n",
      "Epoch:  3\n",
      "Average Loss: 0.313\n",
      "Epoch:  4\n",
      "Average Loss: 0.301\n",
      "Epoch:  5\n",
      "Average Loss: 0.293\n",
      "Epoch:  6\n",
      "Average Loss: 0.287\n",
      "Epoch:  7\n",
      "Average Loss: 0.282\n",
      "Epoch:  8\n",
      "Average Loss: 0.278\n",
      "Epoch:  9\n",
      "Average Loss: 0.274\n",
      "Epoch:  10\n",
      "Average Loss: 0.271\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Analogy Testing",
   "id": "579ac0436671e57f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cbow.vocabulary_index_map.get('book')\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "tokens = ['beach', 'police', 'woman', 'man', 'car', 'crime']\n",
    "vectors = [cbow.input_weights[cbow.vocabulary_index_map.get(token)] for token in tokens]\n",
    "vectors = np.array(vectors)\n",
    "\n",
    "result = np.zeros([6, 6])\n",
    "for i in range(len(vectors)):\n",
    "    for j in range(len(vectors)):\n",
    "        result[i][j] = np.dot(vectors[i], vectors[j]) / (np.linalg.norm(vectors[i]) * np.linalg.norm(vectors[j]))\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(5, 4))\n",
    "axs.imshow(result, cmap='grey')\n",
    "axs.set_xticks(ticks=np.arange(0, 6), labels=tokens)\n",
    "axs.set_yticks(ticks=np.arange(0, 6), labels=tokens)\n",
    "plt.title(\"Cosine similarities between few tokens\")\n",
    "plt.show()"
   ],
   "id": "37dd518003df8e2b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
